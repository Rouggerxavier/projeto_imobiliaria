# Troubleshooting - Guia de Solu√ß√£o de Problemas

## üî¥ Erro: `type=BAD_REQUEST http=400`

### Sintomas
```
ERROR: [LLM_ERROR] type=BAD_REQUEST http=400 provider=openai-compatible model=models/gemini-2.0-flash
```

### Causas Comuns

#### 1. **Formato do modelo incorreto**
‚ùå **ERRADO:** `models/gemini-2.0-flash`
‚úÖ **CORRETO:** `gemini-2.5-flash` ou `gemini-1.5-flash`

**Solu√ß√£o:** Edite o `.env`:
```env
OPENAI_MODEL=gemini-2.5-flash
```

#### 2. **URL da API sem barra final**
‚ùå **ERRADO:** `https://generativelanguage.googleapis.com/v1beta/openai`
‚úÖ **CORRETO:** `https://generativelanguage.googleapis.com/v1beta/openai/`

**Solu√ß√£o:** Edite o `.env`:
```env
OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
```

#### 3. **API Key inv√°lida ou expirada**
- A chave pode ter sido revogada
- Limite de uso foi excedido

**Solu√ß√£o:**
1. Acesse: https://aistudio.google.com/apikey
2. Revogue a chave antiga
3. Gere uma nova
4. Cole no `.env`

#### 4. **response_format n√£o suportado pela Gemini**
O c√≥digo j√° foi corrigido para detectar Gemini e usar instru√ß√£o no system prompt ao inv√©s de `response_format`.

**Confirme que est√° usando a vers√£o atualizada:**
```bash
git pull  # Se estiver em um repo
# Ou verifique se app/agent/llm.py tem a detec√ß√£o de Gemini
```

---

## üü° Erro: Chave API "morre" ap√≥s 1-2 respostas

### Sintomas
- Primeiras mensagens funcionam
- Depois de 1-2 intera√ß√µes, come√ßa a dar erro 400 ou 429

### Causas Prov√°veis

#### 1. **Rate Limit da API (limite de requisi√ß√µes)**
Gemini Free tem limites de:
- **15 RPM** (requests per minute)
- **1 milh√£o TPM** (tokens per minute)

**Solu√ß√£o:**
- Use **Groq** ao inv√©s (limite maior: 30 RPM)
- Configure cooldown no c√≥digo (j√° implementado)

#### 2. **Quota di√°ria excedida**
A API gratuita tem limite di√°rio de tokens.

**Solu√ß√£o:**
- Aguarde 24h para reset
- Ou use Groq/Ollama como alternativa

#### 3. **Timeout muito curto**
Se o timeout for muito curto, pode parecer que a API falhou.

**Solu√ß√£o:** Edite o `.env`:
```env
LLM_TIMEOUT=120  # 2 minutos
```

---

## üü¢ Alternativa Recomendada: Usar Groq

### Por que Groq?
- ‚úÖ Mais est√°vel que Gemini
- ‚úÖ Limite maior (30 RPM)
- ‚úÖ Respostas mais r√°pidas
- ‚úÖ Gratuito

### Como Migrar para Groq

1. **Gere uma chave Groq:**
   - Acesse: https://console.groq.com/keys
   - Crie conta gratuita
   - Gere API key

2. **Edite o `.env`:**
   ```env
   # Comente as linhas do Gemini
   #OPENAI_API_KEY=sua_chave_gemini
   #OPENAI_MODEL=gemini-2.5-flash
   #OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/

   # Descomente as linhas do Groq
   GROQ_API_KEY=sua_chave_groq_aqui
   GROQ_MODEL=llama-3.3-70b-versatile
   ```

3. **Reinicie o servidor:**
   ```bash
   # Ctrl+C para parar
   python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
   ```

---

## üü£ Alternativa Local: Ollama (sem limites!)

### Vantagens
- ‚úÖ 100% local (sem API)
- ‚úÖ Sem limites de rate
- ‚úÖ Privacidade total
- ‚úÖ Funciona offline

### Como Configurar

1. **Instale o Ollama:**
   - Windows/Mac/Linux: https://ollama.ai

2. **Baixe um modelo:**
   ```bash
   ollama pull llama3.2
   # Ou para melhor qualidade:
   ollama pull llama3.1:8b
   ```

3. **Configure o `.env`:**
   ```env
   OPENAI_API_KEY=ollama
   OPENAI_MODEL=llama3.2
   OPENAI_BASE_URL=http://localhost:11434/v1
   LLM_TIMEOUT=120
   ```

4. **Inicie o Ollama (se n√£o iniciou automaticamente):**
   ```bash
   ollama serve
   ```

---

## üîç Debug: Como Verificar se o LLM Est√° Funcionando

### 1. Teste de Conex√£o
```python
# Rode no terminal Python
python -c "from app.agent.llm import test_llm_connection; test_llm_connection()"
```

**Resposta esperada:**
```
[OK] Conex√£o com gemini OK: {'status': 'OK'}
```

### 2. Verifique os Logs
Ao rodar o servidor, procure por:
```
[LLM] Usando Google Gemini com modelo gemini-2.5-flash
```

Se ver `[LLM] Nenhuma API key configurada`, revise o `.env`.

### 3. Teste Manual via cURL
```bash
curl -X POST http://localhost:8000/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "test-123",
    "message": "oi",
    "name": "Teste"
  }'
```

**Resposta esperada:** JSON com uma mensagem do agente.

---

## üìã Checklist de Verifica√ß√£o

Antes de reportar um problema, confirme:

- [ ] Arquivo `.env` existe e est√° configurado
- [ ] API key est√° preenchida (n√£o √© `sua_chave_aqui`)
- [ ] Modelo est√° no formato correto (sem `models/`)
- [ ] URL da API tem barra final `/`
- [ ] Servidor est√° rodando (`python -m uvicorn app.main:app --reload`)
- [ ] Porta 8000 n√£o est√° sendo usada por outro processo
- [ ] `.venv` est√° ativado (`which python` deve mostrar o venv)

---

## üÜò Ainda com Problemas?

### 1. Ative logs detalhados
```env
LOG_LEVEL=debug
```

### 2. Teste com fallback (sem LLM)
```env
USE_LLM=false
```

Se funcionar com `USE_LLM=false`, o problema est√° na configura√ß√£o do LLM.

### 3. Verifique a vers√£o do Python
```bash
python --version  # Deve ser 3.8+
```

### 4. Reinstale depend√™ncias
```bash
pip install --upgrade -r requirements.txt
```

---

## üìñ Logs de Erro Comuns

### `[LLM_ERROR] type=AUTH_INVALID_KEY`
- **Causa:** API key inv√°lida
- **Solu√ß√£o:** Gere nova chave e atualize o `.env`

### `[LLM_ERROR] type=RATE_LIMIT_RPM`
- **Causa:** Excedeu limite de requisi√ß√µes por minuto
- **Solu√ß√£o:** Aguarde 60s ou use Groq

### `[LLM_ERROR] type=QUOTA_EXHAUSTED_DAILY`
- **Causa:** Limite di√°rio excedido
- **Solu√ß√£o:** Aguarde 24h ou mude para Groq/Ollama

### `[LLM_ERROR] type=MODEL_NOT_FOUND`
- **Causa:** Nome do modelo incorreto
- **Solu√ß√£o:** Corrija `OPENAI_MODEL` para `gemini-2.5-flash`

### `[LLM_ERROR] type=NETWORK_TIMEOUT`
- **Causa:** Resposta demorou demais
- **Solu√ß√£o:** Aumente `LLM_TIMEOUT=180`
