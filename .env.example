# === LLM CONFIG ===
# Prioridade: Google Gemini (compat OpenAI) > OpenAI/OpenRouter > Groq/Ollama

# Google Gemini (endpoint compatível com OpenAI)
OPENAI_API_KEY=sua_google_api_key_aqui
OPENAI_MODEL=gemini-2.0-flash
OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai

# OpenAI/OpenRouter (alternativo)
# OPENAI_API_KEY=sua_openai_key_aqui
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1

# Groq/Ollama (fallback)
GROQ_API_KEY=
GROQ_MODEL=
GROQ_BASE_URL=

# Modo apenas triagem (sem busca/listagem de imóveis)
TRIAGE_ONLY=false

# === LLM USAGE ===
# USE_LLM=true  -> Usa LLM (1 chamada por mensagem)
# USE_LLM=false -> Usa fallback determinístico (sem custo de tokens)
USE_LLM=true

# Timeout da chamada LLM (segundos). Aumente para LLM local em CPU.
LLM_TIMEOUT=120

# === LLM LOCAL (Ollama) ===
# Mantem o modelo carregado apos chamadas (ex.: 30m).
LLM_KEEP_ALIVE=30m
# Contexto menor = menos RAM e mais velocidade.
LLM_NUM_CTX=2048
# Use mais threads da CPU (ajuste conforme sua maquina).
LLM_NUM_THREADS=8
# mmap ajuda a reduzir pico de RAM (Windows pode variar).
LLM_USE_MMAP=true
# Faz pre-warm no startup para evitar cold start na 1a requisicao.
LLM_PREWARM=true

# === SERVER ===
PORT=8000
